\documentclass[]{article}

%opening
\Huge\title{STAT 4352 - Mathematical Statistics Notes}
\Large\author{JaimeGoB}
\usepackage[margin=0.5in]{geometry}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{soul}
\setul{}{1pt}

\begin{document}

\maketitle

\newpage
\Huge\section{Chapter 10.2 - Unbiased Estimators}

\Large\textbf{Definition: 10.2 Unbiased Estimator}
\\
A statistic $\hat\theta$ is an \textbf{\textit{Unbiased Estimator}} of parameter $\theta$ if and only if:\newline
\Large\rule{5cm}{0pt} E[$\hat\theta$] = $\theta$\newline
That is $\hat\theta$ on average its value equals $\theta$.
\newline
\newline
\newline
\Large\textbf{Definition: Bias}
\\
\Large\textbf{\textit{Bias}}  of $\hat\theta$: b$_n$($\hat\theta$) = E[$\hat\theta$]$ - \theta$\newline
When:\newline
\Large\rule{2.3cm}{0pt} b$_n$($\hat\theta$) =  E[$\hat\theta$]$ - \theta$ = 0 \textbf{(Unbiased Estimator)}\newline
\Large\rule{2.3cm}{0pt} b$_n$($\hat\theta$) =  E[$\hat\theta$]$ - \theta$ $\neq$ 0 \textbf{(Biased Estimator)}
\newline
\newline
\newline
\Large\textbf{Definition: Asymptotically Unbiased Estimator}
\\
\Large Based on a random sample n, from a given distribution. We say $\hat\theta$ is a \textbf{\textit{Asymptotically Unbiased Estimator}}  if and only if: \newline
 \[ \lim_{n\to\infty} b_n(\hat\theta) = 0\] 
\newline
\newline
\Large\textbf{Properties of Unbiased Estimators}
\begin{itemize}
	\item $\overline{x}$ is always unbiased for all distributions.
	\item  NOT UNIQUE. (there can be multiple unbiased estimators).
	\newline\Large\rule{1cm}{0pt} \textbf{\textit{If you can have multiple unbiased estimators which one is best?}}
	\newline\Large\rule{1cm}{0pt} Next desireable properties are sufficiency and low variance.	
	\item Does not have invariance property.
	\newline\Large\rule{1cm}{0pt} $\overline{x}$ is unbiased for $\mu$ $ \centernot\implies  $ $\overline{x}^2$ is unbiased for $\mu^2$
\end{itemize}


\newpage
\section{Chapter 10.3 - Efficiency}
\Large\textbf{\textit{How to measure accuracy of estimators?}}\newline
\\
\Large\textbf{1) Mean Absolute Error (MAE)}
\\
MAE$_\theta$ = E[ $|\hat\theta - \theta|$ ]
\newline
\newline
\Large\textbf{2) Mean Absolute Deviation (MAD)}
\\
MAD$_\theta$ = median[ $|\hat\theta - \theta|$ ]
\newline
\newline
\Large\textbf{3) Mean Squared Error (MSE)}
\\
MSE$_\theta$ = E$(\hat\theta - \theta)^2$ = Var$_\theta (\hat\theta)$ + Bias$_\theta ^2 (\hat\theta)$ \newline
\newline
\textbf{\textit{For an unbiased estimator (Bias = 0)}} \newline
MSE$_\theta$  = Var$_\theta (\hat\theta)$
\newline
\newline
\newline
\newline
\Large\textbf{Definition: Relative Efficiency}
\\
\Large Let $\hat\theta_1$ and $\hat\theta_2$ be unbiased estimators of $\theta$.
\newline
\newline
\Large\rule{3cm}{0pt} If \Large\rule{.3cm}{0pt}$\dfrac{  Var_\theta (\hat\theta_1)  }{ Var_\theta (\hat\theta_2)}  < $  1
\newline 
\newline
\textbf{\textit{We can say that $\hat\theta_1$ is more efficient than $\hat\theta_2$.}}
\newline 
You would want to pick the estimator $\hat\theta$ that is more efficient (lowest variance). 
\newline
\newline
\newline
\textbf{Efficiency Example 1:}  If \Large\rule{.3cm}{0pt}$\dfrac{  Var_\theta (\hat\theta_1)  }{ Var_\theta (\hat\theta_2)}  = $  0.50 $\Longrightarrow  Var_\theta (\hat\theta_1) = 0.5  Var_\theta (\hat\theta_2) $ 
\newline\newline\Large\rule{10cm}{0pt}  $\hat\theta_1$ is 50\% \textbf{MORE} efficient than $\hat\theta_2$
\newline
\newline
\textbf{Efficiency Example 2:}  If \Large\rule{.3cm}{0pt}$\dfrac{  Var_\theta (\hat\theta_1)  }{ Var_\theta (\hat\theta_2)}  = $  1.50 $\Longrightarrow  Var_\theta (\hat\theta_1) = 1.5  Var_\theta (\hat\theta_2) $ 
\newline\newline\Large\rule{10cm}{0pt}  $\hat\theta_1$ is 50\% \textbf{LESS} efficient than $\hat\theta_2$
\newline\Large\rule{13cm}{0pt}   OR
\newline\Large\rule{10cm}{0pt}  $\hat\theta_2$ is 50\% \textbf{MORE} efficient than $\hat\theta_1$
\newline
\newline
\newline
\newline
\Large\textbf{Definition: Asymptotic Relative Efficiency}
\newline
\Large Based on a random sample n, from a given distribution. We define the comparison of estimators ($\hat\theta_1$,$\hat\theta_2$) is \textbf{\textit{Asymptotically Relative Efficiency}}  when: \newline
\[ ARE = \lim_{n\to\infty}  \dfrac{  Var_\theta (\hat\theta_1)  }{ Var_\theta (\hat\theta_2)}  <  1 \] 
The efficiency(gain) is reduced as sample size ${n\to\infty}$. For huge sample sizes both unbiased estimators are equally good. For small n one estimator is better than other.
\newline
\newline
\Large\textbf{Definition: Uniformly Minimum Variance Unbiased Estimator}
\newline
\Large An unbiased estimator $\hat\theta$ is \textbf{\textit{Uniformly Minimum Variance Unbiased Estimator (UMVUE)}} for $\theta$ if it has the smallest variance in the class of all unbiased estimators for $\theta$.
\newline
\newline
\Large\textbf{Theorem 10.2: Cramer-Rao Inequality}
\newline
\Large It is possible to obtain a lower bound on the variance of all \textbf{\textit{unbiased estimators}} $\theta$.
\begin{itemize}
	\item $\hat\theta$ - unbiased estimator of parameter $\theta$, based on a random sample of n observations.
	\item f(x, $\theta$) is the probability distribution of random variable X.
	\item n is a random sample size
\end{itemize}
\Large The \textbf{\textit{Lower Bound of Variance of an Unbiased Estimator}} is the defined by the Cramer-Rao inequality:
\newline
\newline\Large\rule{3cm}{0pt} $Var (\hat\theta) \geq  \dfrac{  1   }{ I (\theta) }$ \Large\rule{1cm}{0pt} where $I (\theta) = nE \left[\left(  \dfrac{  \partial  }{ \partial\theta } ln  f(X,\theta)     \right)^2 \right] $ 
\newline
\newline
\newline $I (\theta)$ is the Fisher Information in a random sample of size n and $ \dfrac{  \partial  }{ \partial\theta } ln  f(X,\theta)$ is known as score function. It is the smallest possible value variance can have. 
\newline
\newline
\textbf{\textit{UMVUE exists when:}}
\newline
\newline If $Var (\hat\theta) =  \dfrac{  1   }{ I (\theta) }$ It has smallest possible value for variance.
\newline
\newline $\Longrightarrow  \hat\theta$ \textbf{\textit{ is UMVU of }} $ \hat\theta$ 
\newline
\newline
\textbf{\textit{UMVUE does not exists when:}}
\newline
\newline If $Var (\hat\theta) \neq  \dfrac{  1   }{ I (\theta) }$  \textbf{\textit{You can't say $\hat\theta$ is UMVUE as lower bound is not achievable.}}


\newpage
\section{Chapter 10.4 - Consistency}
\Large\textbf{Definition: Consistency}
\\
\Large If $\hat\theta$ is an estimator of $\theta$ based on a random sample of size n, we say that $\hat\theta$ is \textbf{\textit{consistent (closed)}} for $\theta$, if $\epsilon > 0$:   
\newline \[ \lim_{n\to\infty} P( |  \hat\theta - \theta | < \epsilon) = 1   \Large\rule{2cm}{0pt}  \theta - target parameter,  \hat\theta - estimator \] 
\Large\rule{9cm}{0pt}  $\epsilon$ - estimator (small distance ex: 0.0001)
\newline
\newline 
\Large\rule{1cm}{0pt}\textbf{\textit{Consistency is an Asymptotic Property:}}
\newline\Large\rule{1cm}{0pt} Error in estimation using $\hat\theta$ is small 
\newline\Large\rule{1cm}{0pt} $\hat\theta$ converges in probability to $\theta$
\newline\Large\rule{1cm}{0pt} When ${n\to\infty}$ we can be practically certain that the error made with a consistent 
\newline\Large\rule{1cm}{0pt} estimator will be less than any small preassigned positive constant $\epsilon$.
\newline
\newline 
\Large\textbf{Theorem 10.3   }
\\
\Large If $\hat\theta$ is an unbiased estimator of the parameter $\theta$ and  Var($\hat\theta$) $\to 0$ Bias($\hat\theta$) $\to 0$ as  ${n\to\infty}$ then $\hat\theta$ is a consistent estimator of $\theta$.


\newpage
\section{Chapter 10.5 - Sufficiency}
\Large\textbf{Definition: Sufficient Principle}
\begin{itemize}
	\item Reduce data without loosing information about $\theta$.
	\item Captures all information about a sample relevant to estimation of $\theta$, that is, if all the knowledge about $\theta$ that can be gained from the individual sample values and their order can just as well be gained from the value of $\hat\theta$ alone. \newline
\end{itemize}
\Large\textbf{Definition: Sufficient Estimator}
\\
The statistic $\hat\theta$ is a sufficient estimator of parameter $\theta$ of a given distribution \textbf{\textit{iff}} for each value of $\hat\theta$ \ul{\textbf{\textit{the conditional probability distribution or density}}}  of a random sample x$_1$,x$_2$,...x$_n$ given $\hat\theta$ = $\theta$ \ul{\textbf{\textit{is independent of}}} $\theta$. 
\newline
\newline Sufficient property from conditional probability distribution or density when $\hat\theta$ = $\theta$:
\newline 
\newline\Large\rule{5cm}{0pt}  f( x$_1$,x$_2$,...x$_n$ ; $\hat\theta$) = $\dfrac{   f( x_1,x_2,...x_n , \hat\theta)  }{ g(\hat\theta) }$
\newline
\newline Note: \ul{Ratio should not contain $\theta$}  in order to be sufficient estimator of $\theta$
\newline
\newline
\newline
\Large\textbf{Theorem 10.4: Factorization Theorem}
\\
The statistic $\hat\theta$ is a sufficient estimator of the parameter $\theta$ \textbf{\textit{iff}} the joint probability distribution or density of the random sample can be factored so that:
\newline
\newline\Large\rule{5cm}{0pt}  f( x$_1$,x$_2$,...x$_n$ ; $\hat\theta$) = $g(\hat\theta, \theta) * h( x_1,x_2,...x_n )  $
\newline
\newline where \ul{$g(\hat\theta, \theta)$ depends on $\theta$ and $\hat\theta$}  and \ul{$h( x_1,x_2,...x_n )$ does not depend on $\theta$.} 
\newline
\newline \textbf{\textit{Using factorization you want to identify:}}
\begin{itemize}
	\item g function $\implies$ function $\theta$
	\item h function $\implies$ function without $\theta$ \newline (h(x) = 1 if not present)
\end{itemize}
\textbf{Properties of Sufficiency:}
\begin{itemize}
	\item Complete  data is always sufficient.
	\item \ul{Any 1-1 function of a sufficient statistic is also sufficient.}
	\item Good estimators should be functions of sufficient statistic. \newline\textbf{(a good estimator is sufficient)} 
\end{itemize}

\newpage
\section{Chapter 10.8 - Method of Maximum Likelihood}
\Large\textbf{Notation}
\newline X = ( X$_1$,X$_2$,...X$_n$ )  $\stackrel{i.i.d.}{\sim}$ f$_\theta$(x)  \newline Data before observed - r.v.'s with same distribution.
\newline $\theta$ may be a vector, $\theta \in \Theta$
\newline $\Theta$ is parameter space.  Ex: parameter space of $\mathcal{N}(\mu,\,\sigma^{2})$ is $-\infty < \mu < \infty$, $-\infty < \sigma^2 < \infty$,
\newline x = (x$_1$,x$_2$,...x$_n$ )   Data that has been observed. (Set of sample elements)
\newline
\newline 
\Large\textbf{Definition: Likelihood Function}
\newline Joint pdf/pmf of X considered as a function of $\theta$ keeping the data X fixed.
\newline
\newline $\mathcal{L}$($\theta$) = $\displaystyle\prod_{i=1}^{n} f_\theta$(x$_i$)  =  f( x$_1$,x$_2$,...x$_n$ ; $\hat\theta$) = f(x$_1$,$\hat\theta$)f(x$_2$,$\hat\theta$)...f(x$_n$,$\hat\theta$)
\newline
\newline vary $\theta$ to find value that maximizes product, this value for $\theta$ is known as MLE.
\newline
\newline
\Large\textbf{Definition: Maximum Likelihood Estimator}
\newline The Maximum Likelihood Estimator (MLE) of $\theta$ is the value $\theta$ that maximizes the Likelihood function $\mathcal{L}$($\theta$).
\begin{itemize}
	\item Complete  data is always sufficient.
	\item Value of $\theta$ that maximizes $\mathcal{L}$($\theta$) also maximizes $\log\mathcal{L}$($\theta$)/$\ln\mathcal{L}$($\theta$). 	
	\item First Derivative: $\dfrac{\partial\mathcal{L}(\theta)  }{\partial\theta} \implies$ Critical Points (Max/Min)
	\item Second Derivative: $\dfrac{\partial^2\mathcal{L}(\theta)  }{\partial\theta^2} < $ 0 $\implies$  Maximum exists.\newline
\end{itemize}
\Large\textbf{Log-Likelihood Function log$\mathcal{L}$($\theta$)/ln$\mathcal{L}$($\theta$)}
\newline Because log/ln is a monotone function, if we \textbf{\textit{maximize log-Likelihood it is the same as maximizing Likelihood.} } The reason why  because \ul{$\log\mathcal{L}$($\theta$)/$\ln\mathcal{L}$($\theta$) is used because taking the derivative is much easier.} When referring to log we mean log$_e$ = ln.
\begin{itemize}
	\item $\log(ab)$ = $\log(a)$ + $\log(b)$
	\newline Ex: $f(x) = \displaystyle\prod_{i=1}^{n}g(x_i) \implies \ln{f(x)} =  \sum_{i=1}^{n} \ln g(x_i)$ 
	\newline where x = (x$_1$,x$_2$,...x$_n$ )  Set of sample elements
\end{itemize}
This property applies to both log/ln.\textbf{\textit{The inner product can be expressed as a sum of individual elements.}} This comes super handy when taking derivatives.

\newpage
\Large\textbf{Properties of a Maximum Likelihood Estimator}
\begin{itemize}
	\item $\hat\theta_{MLE}$ is always a function of sufficient statistics whenever they exist.
	\item Optimal when n is large.
	\item May not be good when the distribution assumptions are wrong.
	\item $\hat\theta_{MLE} \in \Theta$ (MLE is included in parameter space)
	\item $\hat\theta_{MLE}$ has invariance property:
	\newline\Large\rule{7cm}{0pt} $\hat\theta$ is MLE for $\theta$ 
	\newline\Large\rule{8cm}{0pt} $\iff$ 
	\newline\Large\rule{7cm}{0pt} $\hat\theta^2$ is MLE for $\theta^2$ 
	\newline\Large\rule{8cm}{0pt} $\iff$ 
	\newline\Large\rule{8cm}{0pt} ........
\end{itemize}


\newpage
\section{10.9 Bayesian Inference}
\Large\textbf{Bayes Rule} \newline
Conditional probability can be rewritten with a Bayes Rules.
\newline
\newline
P(A $|$ B ) = $\dfrac{P(A \cap B)}{ P(B) }$ = $\dfrac{P( B  |  A) P(A)}{ P(B) }$ 
\newline
\newline
\Large\textbf{Difference Between Classical and Bayesian Approach}
\newline
\textbf{Classical Approach:}
\begin{itemize}
	\item $\theta$ is a \textul{unknown parameter} and \textit{fixed.}
	\item Does not have a probability distribution f($\theta$)
\end{itemize}
\textbf{Bayesian Approach:}
\begin{itemize}
	\item $\theta$ is a \textul{random variable} and \textit{not fixed.}
	\item Has a probability distribution f($\theta$)
\end{itemize}
\textbf{Prior Distribution for $\theta$:}
\begin{itemize}
	\item \textul{Denoted f($\theta$) or $\pi$($\theta$)}
	\item Specified before seeing data.
	\item \textul{Reflects personal degree of belief about what are the possible values of $\theta$ and how likely they are.}
	\item Can be discrete or continuous.
	\item Can be vague: \textit{All values equally likely}
\end{itemize}
Let data X = (X$_1$, X$_2$,...,X$_n$) be a random sample from a population. \newline
The distribution of random variable X , will have a pdf/pmf: f(\textbf{x}$|$$\theta$)  "distribution depends on $\theta$" will have a \textul{\textbf{joint density of likelihood of sample}}  is:
\newline
\newline\Large\rule{5cm}{0pt} f(x$_1$, x$_2$,...,x$_n | \theta$) =  f(x$_1 | \theta$)f(x$_2 | \theta$)...f(x$_n | \theta$)

\newpage
\Large\textbf{Posterior Distribution of $\theta$}
\newline
We define posterior distribution of $\theta$ as the \textul{conditional distribution of $\theta$ given the sample results.}
\newline
\newline\Large\rule{2.2cm}{0pt}   f($\theta |$ x$_1$, x$_2$,...,x$_n$) = $\dfrac{f(x_1, x_2,...,x_n ; \theta)}{ f(x_1, x_2,...,x_n)} = \dfrac{f(x_1, x_2,...,x_n | \theta)f(\theta)}{ \int_{-\infty}^{\infty}  f(x_1, x_2,...,x_n | \theta)f(\theta)  \,d\theta  }$
\newline
\newline where,
\newline
\newline  \Large\rule{2cm}{0pt}  \textbf{Joint Distribution } \Large\rule{0cm}{0pt} \textbf{Joint Density of } 
\newline  \Large\rule{2cm}{0pt}  \textbf{of Sample and} $\theta$ \Large\rule{.65cm}{0pt} \textbf{Likelihood of Sample} \Large\rule{0cm}{0pt} \textbf{Prior}
\newline\Large\rule{2cm}{0pt} $f(x_1, x_2,...,x_n ; \theta)$     \Large\rule{.3cm}{0pt} =   \Large\rule{.5cm}{0pt}  f(x$_1$, x$_2$,...,x$_n | \theta$)  \Large\rule{1.5cm}{0pt}   f($\theta)$ 
\newline
\newline \Large\rule{2cm}{0pt}  \textbf{Marginal Distribution of Sample} ( independent of $\theta$)
\newline  \Large\rule{2cm}{0pt}  Continuous Case:
\Large\rule{0cm}{0pt}  \[f(x_1, x_2,...,x_n ) =  \int_{-\infty}^{\infty}  f(x_1, x_2,...,x_n | \theta)f(\theta)  \,d\theta \]
In summary,
\newline \Large\rule{2.5cm}{0pt} \textit{Posterior Distribution = Likelihood * Prior / Normalizing Constant}
\newline
\newline \textul{Posterior Distribution of a parameter can be used to: }
\begin{itemize}
	\item make estimates
	\item make probability statements about the parameter.
\end{itemize}
\Large\textbf{Def: Conjugate Family}
\newline When prior and posterior distribution belong to the same distribution family. 
\newline
\newline
\Large\textbf{NOTES:}
\begin{itemize}
	\item Because denominator of Posterior Distribution is a normalizing constant and independent of $\theta$: 
	\[  \dfrac{1}{f(x_1, x_2,...,x_n )}   \]
	it is known as \textit{proportional factor} and it \textul{will get absorbed in the} $\propto$ sign. 
	\newline\newline \textbf{\textul{ We can summarize the Posterior Distribution to:}}
	\[  f(\theta | x_1, x_2,...,x_n)  \propto f(x_1, x_2,...,x_n | \theta) f( \theta)  \]
	\Large\rule{3.3cm}{0pt} \textit{Posterior Distribution $\propto$ \Large\rule{.2cm}{0pt}  Likelihood * Prior }
\end{itemize}

\newpage
\begin{itemize}
	\item The Posterior Distribution depends only sufficient statistics. 
	\newline
	\newline \Large\rule{1.3cm}{0pt} Let T(x) be a sufficient static for $\theta$ using \textit{factorization theorem} :
	\[  f(\textbf{x}| \theta  )  =  g(T(\textbf{x}), \theta) h(\textbf{x}) \]
	\newline \Large\rule{1.3cm}{0pt}  We can conclude: 
	\[  f(\theta | \textbf{x})  \propto f(\textbf{x} | \theta) f( \theta)  \]
	\Large\rule{1.3cm}{0pt}\textit{factorization theorem} 
	\[  f(\theta | \textbf{x})  \propto h(\textbf{x}) g(T(\textbf{x})| \theta)  f( \theta)  \]
	\Large\rule{1.3cm}{0pt}\textit{h(\textbf{x}) does not depend on $\theta$ so constant gets absorbed in $\propto$}  
	\[  f(\theta | \textbf{x})  \propto g(T(\textbf{x})| \theta)  f( \theta)  \]
	$\implies$ f($\theta | \textbf{x} $)  = $f(\theta | T(\textbf{x}))$
	\newline\newline Beliefs of $\theta$ having observed full data \textbf{x} are same as if we had observed only the sufficient statistic T(\textbf{x}).
\end{itemize}'
\newline \Large\textbf{Properties of Bayes Estimator}
\begin{itemize}
	\item Depends on Sufficient Statistic.
	\item Under certain circumstances, as n$\to\infty$ it is equivalent to MLE.
	\item Effect of prior diminishes as data dominates (n$\to\infty$) .
	\item Optimal for large n.
	\item Posterior Distribution can be estimated for non-conjugate priors using Markov Chains or Monte Carlo.
\end{itemize}'

\newpage
\section{Chapter number - Chapter Name}
\Large\textbf{Theorem number: Theorem Name}






\end{document}
